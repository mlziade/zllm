# zllm

## Description

zllm is my implementation of a server with a LLM API that runs models locally on a VPS.

It uses the Golang framework [Fiber](https://github.com/gofiber/fiber) to integrate with [Ollama](https://github.com/ollama/ollama) running as a [server](https://github.com/ollama/ollama?tab=readme-ov-file#building).

## Technologies

### Backend

- Golang
- Fiber
- Ollama
- Tesseract

### Deployment Infrastructure

- Ubuntu: Linux server for hosting the application
- Nginx: Web server acting as a reverse proxy
- Systemd: Linux initialization system used to automate application startup and manage service processes
- Unix Socket: For communication between Gunicorn and Nginx

## API Documentation

Refer to the [API documentation](docs/endpoints.md) for detailed information about endpoints and authentication.

## Roadmap

### Completed
- âœ… Basic API server with Fiber framework
- âœ… JWT authentication with role-based access
- âœ… Integration with Ollama for local model execution
- âœ… Text generation endpoints (streaming and non-streaming)
- âœ… Model management (list, add)
- âœ… OCR capabilities for text extraction from images (Tesseract)

### Future
- ðŸš§ Multimodal models support (text + image inputs)
- ðŸš§ Translation endpoint for multiple languages
- ðŸš§ Image generation