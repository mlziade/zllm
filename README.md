# zllm

## Description

zllm is my implementation of a server with a LLM API that runs models locally on a VPS.

It uses the Golang framework [Fiber](https://github.com/gofiber/fiber) to integrate with [Ollama](https://github.com/ollama/ollama) running as a [server](https://github.com/ollama/ollama?tab=readme-ov-file#building).

## Technologies

### Backend

- Golang
- Fiber
- Ollama
- SQLite with GORM ORM
- JWT Authentication

### Deployment Infrastructure

- **Ubuntu**: Linux server for hosting the application
- **Nginx**: Web server acting as a reverse proxy
- **Systemd**: Service manager for the Fiber application daemon
- **Unix Socket**: Communication between Nginx and the Go Fiber server

### CI/CD

- **GitHub Actions**: Automated continuous integration and deployment pipeline

## API Documentation

Refer to the [API documentation](docs/endpoints.md) for detailed information about endpoints and authentication.

## Roadmap

### Completed
- ‚úÖ Basic API server with Fiber framework
- ‚úÖ JWT authentication with role-based access
- ‚úÖ Integration with Ollama for local model execution
- ‚úÖ Text generation endpoints (streaming and non-streaming)
- ‚úÖ Model management (list, add, remove)
- ‚ùå OCR capabilities for text extraction from images (Tesseract) *[Deprecated in favor of multimodal models]*
- ‚úÖ OCR capabilities for text extraction from images (Multimodal Models)
- ‚úÖ Asynchronous Jobs implementation
- ‚úÖ Asynchronous Generation Jobs
- ‚úÖ Asynchronous Multimodal Extraction


### Projected
- üöß Non multimodal LLM OCR Implementation for images and pdfs
- üöß Translation endpoint for multiple languages (Multimodal models)
- üöß Image generation
- ‚úÖ Automated CI/CD workflow using GitHub Actions and SSH deploy